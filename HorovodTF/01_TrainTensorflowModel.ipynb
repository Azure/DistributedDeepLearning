{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TensorFlow Model Distributed on Batch AI\n",
    "In this notebook we will train a TensorFlow model ([ResNet50](https://arxiv.org/abs/1512.03385)) in a distributed fashion using [Horovod](https://github.com/uber/horovod) on the Imagenet dataset. This tutorial will take you through the following steps:\n",
    " * [Create Experiment](#experiment)\n",
    " * [Upload Training Scripts](#training_scripts)\n",
    " * [Submit and Monitor Job](#job)\n",
    " * [Clean Up Resources](#clean_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../common\") \n",
    "\n",
    "import json\n",
    "from dotenv import get_key\n",
    "import os\n",
    "from utils import write_json_to_file, dotenv_for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the USE_FAKE to True if you want to use fake data rather than the ImageNet dataset. This is often a good way to debug your models as well as checking what IO overhead is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Variables for Batch AI - change as necessary\n",
    "dotenv_path = dotenv_for()\n",
    "GROUP_NAME             = get_key(dotenv_path, 'GROUP_NAME')\n",
    "FILE_SHARE_NAME        = get_key(dotenv_path, 'FILE_SHARE_NAME')\n",
    "WORKSPACE              = get_key(dotenv_path, 'WORKSPACE')\n",
    "NUM_NODES              = int(get_key(dotenv_path, 'NUM_NODES'))\n",
    "CLUSTER_NAME           = get_key(dotenv_path, 'CLUSTER_NAME')\n",
    "GPU_TYPE               = get_key(dotenv_path, 'GPU_TYPE')\n",
    "PROCESSES_PER_NODE     = int(get_key(dotenv_path, 'PROCESSES_PER_NODE'))\n",
    "STORAGE_ACCOUNT_NAME   = get_key(dotenv_path, 'STORAGE_ACCOUNT_NAME')\n",
    "\n",
    "EXPERIMENT             = f\"distributed_tensorflow_{GPU_TYPE}\"\n",
    "USE_FAKE               = False\n",
    "DOCKERHUB              = os.getenv('DOCKER_REPOSITORY', \"masalvar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE='-x FAKE=True' if USE_FAKE else ''\n",
    "TOTAL_PROCESSES = PROCESSES_PER_NODE * NUM_NODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='experiment'></a>\n",
    "# Create Experiment\n",
    "Next we create our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"creationTime\": \"2018-12-17T13:19:30.658000+00:00\",\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchdtdemorg/providers/Microsoft.BatchAI/workspaces/workspace/experiments/distributed_pytorch_v100\",\n",
      "  \"name\": \"distributed_pytorch_v100\",\n",
      "  \"provisioningState\": \"succeeded\",\n",
      "  \"provisioningStateTransitionTime\": \"2018-12-17T13:19:30.658000+00:00\",\n",
      "  \"resourceGroup\": \"batchdtdemorg\",\n",
      "  \"type\": \"Microsoft.BatchAI/workspaces/experiments\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az batchai experiment create -n $EXPERIMENT -g $GROUP_NAME -w $WORKSPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_scripts'></a>\n",
    "# Upload Training Scripts\n",
    "We need to upload our training scripts and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = !az storage account keys list -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME\n",
    "storage_account_key = json.loads(''.join([i for i in json_data if 'WARNING' not in i]))[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AZURE_STORAGE_ACCOUNT=batchdtdemost\n",
      "env: AZURE_STORAGE_KEY=AtQA2uvmxTSvo0SXnI5FjMOXl+qp5fKwNcPL+Y2N0N/0+EhcRt4RhFuXf+YKvG9qDSrB6ZrgNmJ8fgloABMtSQ==\n"
     ]
    }
   ],
   "source": [
    "%env AZURE_STORAGE_ACCOUNT $STORAGE_ACCOUNT_NAME\n",
    "%env AZURE_STORAGE_KEY=$storage_account_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload our training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az storage file upload --share-name $FILE_SHARE_NAME --source src/imagenet_estimator_tf_horovod.py --path scripts\n",
    "!az storage file upload --share-name $FILE_SHARE_NAME --source src/resnet_model.py --path scripts\n",
    "!az storage file upload --share-name $FILE_SHARE_NAME --source ../common/timer.py --path scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our cluster we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name    Resource Group    Workspace    VM Size             State    Idle    Running    Preparing    Leaving    Unusable\r\n",
      "------  ----------------  -----------  ------------------  -------  ------  ---------  -----------  ---------  ----------\r\n",
      "msv100  batchdtdemorg     workspace    STANDARD_NC24RS_V3  steady   2       0          0            0          0\r\n"
     ]
    }
   ],
   "source": [
    "!az batchai cluster list -w $WORKSPACE -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='job'></a>\n",
    "## Submit and Monitor Job\n",
    "Below we specify the job we wish to execute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_dict = {\n",
    "  \"$schema\": \"https://raw.githubusercontent.com/Azure/BatchAI/master/schemas/2017-09-01-preview/job.json\",\n",
    "  \"properties\": {\n",
    "    \"nodeCount\": NUM_NODES,\n",
    "    \"customToolkitSettings\": {\n",
    "      \"commandLine\": f\"echo $AZ_BATCH_HOST_LIST; \\\n",
    "    cat $AZ_BATCHAI_MPI_HOST_FILE; \\\n",
    "    mpirun -np {TOTAL_PROCESSES} --hostfile $AZ_BATCHAI_MPI_HOST_FILE \\\n",
    "    -bind-to none -map-by slot \\\n",
    "    -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH \\\n",
    "    -mca btl_tcp_if_include eth0 \\\n",
    "    -x NCCL_SOCKET_IFNAME=eth0 \\\n",
    "    -mca btl ^openib \\\n",
    "    -x NCCL_IB_DISABLE=1 \\\n",
    "    -x DISTRIBUTED=True \\\n",
    "    -x AZ_BATCHAI_INPUT_TRAIN \\\n",
    "    -x AZ_BATCHAI_INPUT_TEST \\\n",
    "    --allow-run-as-root \\\n",
    "      {FAKE} \\\n",
    "      python -u $AZ_BATCHAI_INPUT_SCRIPTS/imagenet_estimator_tf_horovod.py\"\n",
    "    },\n",
    "    \"stdOutErrPathPrefix\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs\",\n",
    "    \"inputDirectories\": [{\n",
    "        \"id\": \"SCRIPTS\",\n",
    "        \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs/scripts\"\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"TRAIN\",\n",
    "        \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/nfs/imagenet\",\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"TEST\",\n",
    "        \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/nfs/imagenet\",\n",
    "      },\n",
    "    ],\n",
    "    \"outputDirectories\": [{\n",
    "        \"id\": \"MODEL\",\n",
    "        \"pathPrefix\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs\",\n",
    "        \"pathSuffix\": \"Models\"\n",
    "    }],\n",
    "    \"containerSettings\": {\n",
    "      \"imageSourceRegistry\": {\n",
    "        \"image\": f\"{DOCKERHUB}/caia-horovod-tensorflow\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file(jobs_dict, 'job.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME='tensorflow-horovod-{}'.format(NUM_NODES*PROCESSES_PER_NODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now submit the job to Batch AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"caffe2Settings\": null,\n",
      "  \"caffeSettings\": null,\n",
      "  \"chainerSettings\": null,\n",
      "  \"cluster\": {\n",
      "    \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchdtdemorg/providers/Microsoft.BatchAI/workspaces/workspace/clusters/msv100\",\n",
      "    \"resourceGroup\": \"batchdtdemorg\"\n",
      "  },\n",
      "  \"cntkSettings\": null,\n",
      "  \"constraints\": {\n",
      "    \"maxWallClockTime\": \"7 days, 0:00:00\"\n",
      "  },\n",
      "  \"containerSettings\": {\n",
      "    \"imageSourceRegistry\": {\n",
      "      \"credentials\": null,\n",
      "      \"image\": \"masalvar/caia-horovod-pytorch\",\n",
      "      \"serverUrl\": null\n",
      "    },\n",
      "    \"shmSize\": null\n",
      "  },\n",
      "  \"creationTime\": \"2018-12-17T13:47:50.202000+00:00\",\n",
      "  \"customMpiSettings\": null,\n",
      "  \"customToolkitSettings\": {\n",
      "    \"commandLine\": \"echo $AZ_BATCH_HOST_LIST;     cat $AZ_BATCHAI_MPI_HOST_FILE;     mpirun -np 8 --hostfile $AZ_BATCHAI_MPI_HOST_FILE     -bind-to none -map-by slot     -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH     -mca btl_tcp_if_include eth0     -x NCCL_SOCKET_IFNAME=eth0     -mca btl ^openib     -x NCCL_IB_DISABLE=1     -x DISTRIBUTED=True     -x AZ_BATCHAI_INPUT_TRAIN     -x AZ_BATCHAI_INPUT_TEST     --allow-run-as-root       -x FAKE=True       python -u $AZ_BATCHAI_INPUT_SCRIPTS/imagenet_pytorch_horovod.py\"\n",
      "  },\n",
      "  \"environmentVariables\": null,\n",
      "  \"executionInfo\": {\n",
      "    \"endTime\": null,\n",
      "    \"errors\": null,\n",
      "    \"exitCode\": null,\n",
      "    \"startTime\": \"2018-12-17T13:47:54.570000+00:00\"\n",
      "  },\n",
      "  \"executionState\": \"running\",\n",
      "  \"executionStateTransitionTime\": \"2018-12-17T13:47:54.570000+00:00\",\n",
      "  \"horovodSettings\": null,\n",
      "  \"id\": \"/subscriptions/edf507a2-6235-46c5-b560-fd463ba2e771/resourceGroups/batchdtdemorg/providers/Microsoft.BatchAI/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8\",\n",
      "  \"inputDirectories\": [\n",
      "    {\n",
      "      \"id\": \"SCRIPTS\",\n",
      "      \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs/scripts\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"TRAIN\",\n",
      "      \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/nfs/imagenet\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"TEST\",\n",
      "      \"path\": \"$AZ_BATCHAI_MOUNT_ROOT/nfs/imagenet\"\n",
      "    }\n",
      "  ],\n",
      "  \"jobOutputDirectoryPathSegment\": \"edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/43d5d58c-2ecd-4aa4-a459-93be2f302b7e\",\n",
      "  \"jobPreparation\": null,\n",
      "  \"mountVolumes\": null,\n",
      "  \"name\": \"pytorch-horovod-8\",\n",
      "  \"nodeCount\": 2,\n",
      "  \"outputDirectories\": [\n",
      "    {\n",
      "      \"id\": \"MODEL\",\n",
      "      \"pathPrefix\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs\",\n",
      "      \"pathSuffix\": \"Models\"\n",
      "    }\n",
      "  ],\n",
      "  \"provisioningState\": \"succeeded\",\n",
      "  \"provisioningStateTransitionTime\": \"2018-12-17T13:47:50.484000+00:00\",\n",
      "  \"pyTorchSettings\": null,\n",
      "  \"resourceGroup\": \"batchdtdemorg\",\n",
      "  \"schedulingPriority\": \"normal\",\n",
      "  \"secrets\": null,\n",
      "  \"stdOutErrPathPrefix\": \"$AZ_BATCHAI_MOUNT_ROOT/extfs\",\n",
      "  \"tensorFlowSettings\": null,\n",
      "  \"toolType\": \"custom\",\n",
      "  \"type\": \"Microsoft.BatchAI/workspaces/experiments/jobs\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az batchai job create -n $JOB_NAME --cluster $CLUSTER_NAME -w $WORKSPACE -e $EXPERIMENT -f job.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the command below we can check the status of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name               Cluster    Cluster RG     Cluster Workspace    Tool    Nodes    State    Exit code\r\n",
      "-----------------  ---------  -------------  -------------------  ------  -------  -------  -----------\r\n",
      "pytorch-horovod-8  msv100     batchdtdemorg  workspace            custom  2        running\r\n"
     ]
    }
   ],
   "source": [
    "!az batchai job list -w $WORKSPACE -e $EXPERIMENT -o table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the files that the job has generated use the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"contentLength\": 9925,\r\n",
      "    \"downloadUrl\": \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/ffa69c05-3f59-41b3-bfc4-370ad3022d9a/stdouterr/execution-tvm-829305193_1-20181217t125904z.log?sv=2016-05-31&sr=f&sig=RDpy9UMuftOa1w2TM6fROekEqc6ISPRmAwsoQufRzig%3D&se=2018-12-17T14%3A29%3A33Z&sp=rl\",\r\n",
      "    \"fileType\": \"file\",\r\n",
      "    \"lastModified\": \"2018-12-17T13:26:52+00:00\",\r\n",
      "    \"name\": \"execution-tvm-829305193_1-20181217t125904z.log\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"contentLength\": 14343,\r\n",
      "    \"downloadUrl\": \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/ffa69c05-3f59-41b3-bfc4-370ad3022d9a/stdouterr/execution-tvm-829305193_2-20181217t125904z.log?sv=2016-05-31&sr=f&sig=i1S6%2BgVSpK%2BX1o%2BXOLuNBFJ%2FZrRK8W1d7ZEbR8a1NJU%3D&se=2018-12-17T14%3A29%3A33Z&sp=rl\",\r\n",
      "    \"fileType\": \"file\",\r\n",
      "    \"lastModified\": \"2018-12-17T13:26:53+00:00\",\r\n",
      "    \"name\": \"execution-tvm-829305193_2-20181217t125904z.log\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"contentLength\": 80,\r\n",
      "    \"downloadUrl\": \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/ffa69c05-3f59-41b3-bfc4-370ad3022d9a/stdouterr/stderr.txt?sv=2016-05-31&sr=f&sig=%2BeJhQCBEu4tCdRiUMpNfAd2fwtuOFLacYWBQdsGp80g%3D&se=2018-12-17T14%3A29%3A33Z&sp=rl\",\r\n",
      "    \"fileType\": \"file\",\r\n",
      "    \"lastModified\": \"2018-12-17T13:26:53+00:00\",\r\n",
      "    \"name\": \"stderr.txt\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"contentLength\": 988,\r\n",
      "    \"downloadUrl\": \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/ffa69c05-3f59-41b3-bfc4-370ad3022d9a/stdouterr/stdout.txt?sv=2016-05-31&sr=f&sig=pua2p8PQ0h5VE51Nn%2BBgUrw7rXP8HuDlSf75MDqxJ84%3D&se=2018-12-17T14%3A29%3A33Z&sp=rl\",\r\n",
      "    \"fileType\": \"file\",\r\n",
      "    \"lastModified\": \"2018-12-17T13:27:28+00:00\",\r\n",
      "    \"name\": \"stdout.txt\"\r\n",
      "  }\r\n",
      "]\r\n"
     ]
    }
   ],
   "source": [
    "!az batchai job file list -w $WORKSPACE -e $EXPERIMENT --j $JOB_NAME --output-directory-id stdouterr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also able to stream the stdout and stderr that our job produces. This is great to check the progress of our job as well as debug issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "stripout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mFile found with URL \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/43d5d58c-2ecd-4aa4-a459-93be2f302b7e/stdouterr/stdout.txt?sv=2016-05-31&sr=f&sig=%2F3GTrC%2BW73ccZZ82QFvYxHmjrayV0pquj6EYSeqC%2B0I%3D&se=2018-12-17T14%3A51%3A24Z&sp=rl\". Start streaming\u001b[0m\n",
      "10.0.0.5,10.0.0.6\n",
      "10.0.0.5 slots=4 max-slots=4\n",
      "10.0.0.6 slots=4 max-slots=4\n",
      "INFO:__main__:1:  Runnin Distributed\n",
      "INFO:__main__:0:  Runnin Distributed\n",
      "INFO:__main__:5:  Runnin Distributed\n",
      "INFO:__main__:2:  Runnin Distributed\n",
      "INFO:__main__:3:  Runnin Distributed\n",
      "INFO:__main__:6:  Runnin Distributed\n",
      "INFO:__main__:7:  Runnin Distributed\n",
      "INFO:__main__:4:  Runnin Distributed\n",
      "INFO:__main__:0:  PyTorch version 0.4.0\n",
      "INFO:__main__:0:  Setting up fake loaders\n",
      "INFO:__main__:2:  PyTorch version 0.4.0\n",
      "INFO:__main__:2:  Setting up fake loaders\n",
      "INFO:__main__:1:  PyTorch version 0.4.0\n",
      "INFO:__main__:1:  Setting up fake loaders\n",
      "INFO:__main__:3:  PyTorch version 0.4.0\n",
      "INFO:__main__:3:  Setting up fake loaders\n",
      "INFO:__main__:4:  PyTorch version 0.4.0\n",
      "INFO:__main__:4:  Setting up fake loaders\n",
      "INFO:__main__:5:  PyTorch version 0.4.0\n",
      "INFO:__main__:5:  Setting up fake loaders\n",
      "INFO:__main__:7:  PyTorch version 0.4.0\n",
      "INFO:__main__:7:  Setting up fake loaders\n",
      "INFO:__main__:6:  PyTorch version 0.4.0\n",
      "INFO:__main__:6:  Setting up fake loaders\n",
      "INFO:__main__:0:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:0:  Loading model\n",
      "INFO:__main__:3:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:3:  Loading model\n",
      "INFO:__main__:1:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:1:  Loading model\n",
      "INFO:__main__:2:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:2:  Loading model\n",
      "INFO:__main__:4:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:4:  Loading model\n",
      "INFO:__main__:7:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:7:  Loading model\n",
      "INFO:__main__:6:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:6:  Loading model\n",
      "INFO:__main__:5:  Creating fake data 1000 labels and 640 images\n",
      "INFO:__main__:5:  Loading model\n",
      "INFO:__main__:0:  Training ...\n",
      "INFO:__main__:2:  Training ...\n",
      "INFO:__main__:3:  Training ...\n",
      "INFO:__main__:1:  Training ...\n",
      "INFO:__main__:6:  Training ...\n",
      "INFO:__main__:7:  Training ...\n",
      "INFO:__main__:5:  Training ...\n",
      "INFO:__main__:4:  Training ...\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO NET : Using interface eth0:10.0.0.5<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO NET/Socket : 1 interfaces found\n",
      "NCCL version 2.2.13+cuda9.0\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000000:469:572 [3] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000000:469:572 [3] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000001:450:552 [3] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000001:450:552 [3] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO Using internal Network Socket\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO Using NCCL Low-latency algorithm for sizes below 16384\n",
      "974d10b55464480199a0bc9c4b4615cf000001:450:552 [3] INFO comm 0x7f97042d7c90 rank 7 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000001:450:552 [3] INFO NET : Using interface eth0:10.0.0.6<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000001:450:552 [3] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO comm 0x7f9474347240 rank 4 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO NET : Using interface eth0:10.0.0.6<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO comm 0x7f4d7827fe00 rank 0 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO comm 0x7f1e6c2c6850 rank 5 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO NET : Using interface eth0:10.0.0.6<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000000:469:572 [3] INFO comm 0x7f26e0290fb0 rank 3 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000000:469:572 [3] INFO NET : Using interface eth0:10.0.0.5<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000000:469:572 [3] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO comm 0x7ff4ac2c1f70 rank 6 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO NET : Using interface eth0:10.0.0.6<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO comm 0x7f4ddc271930 rank 1 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO NET : Using interface eth0:10.0.0.5<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO comm 0x7f039025d7c0 rank 2 nranks 8\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO NET : Using interface eth0:10.0.0.5<0>\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO NET/Socket : 1 interfaces found\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Using 256 threads\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Min Comp Cap 7\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO NCCL_SINGLE_RING_THRESHOLD=262144\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Ring 00 :    0   1   2   3   4   5   6   7\n",
      "974d10b55464480199a0bc9c4b4615cf000001:448:553 [1] INFO 5[448] -> 6[449] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO 3 -> 4 via NET/Socket/0\n",
      "974d10b55464480199a0bc9c4b4615cf000001:447:554 [0] INFO 4[447] -> 5[448] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000001:449:551 [2] INFO 6[449] -> 7[450] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO 7 -> 0 via NET/Socket/0\n",
      "974d10b55464480199a0bc9c4b4615cf000000:467:571 [1] INFO 1[467] -> 2[468] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO 0[466] -> 1[467] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000000:468:570 [2] INFO 2[468] -> 3[469] via direct shared memory\n",
      "974d10b55464480199a0bc9c4b4615cf000000:466:579 [0] INFO Launch mode Parallel\n",
      "INFO:__main__:3: [Epoch 0]  duration(6.689605174999997)  loss:7.052699089050293 total-samples: 0\n",
      "INFO:__main__:7: [Epoch 0]  duration(6.689712283000063)  loss:7.05151891708374 total-samples: 0\n",
      "INFO:__main__:6: [Epoch 0]  duration(6.691189228999974)  loss:6.988680362701416 total-samples: 0\n",
      "INFO:__main__:5: [Epoch 0]  duration(6.692733973000031)  loss:6.947587013244629 total-samples: 0\n",
      "INFO:__main__:1: [Epoch 0]  duration(6.693754228999751)  loss:7.041652679443359 total-samples: 0\n",
      "INFO:__main__:2: [Epoch 0]  duration(6.697406198999943)  loss:7.09742546081543 total-samples: 0\n",
      "INFO:__main__:0: [Epoch 0]  duration(6.699599520999982)  loss:7.070676803588867 total-samples: 0\n",
      "INFO:__main__:4: [Epoch 0]  duration(6.695543971000006)  loss:6.981583118438721 total-samples: 0\n",
      "INFO:__main__:3: [Epoch 0]  duration(57.50169312499975)  loss:5.711731433868408 total-samples: 6400\n",
      "INFO:__main__:5: [Epoch 0]  duration(57.49870837199978)  loss:5.794982433319092 total-samples: 6400\n",
      "INFO:__main__:7: [Epoch 0]  duration(57.50195155399979)  loss:5.764939785003662 total-samples: 6400\n",
      "INFO:__main__:6: [Epoch 0]  duration(57.50037861200008)  loss:5.75480318069458 total-samples: 6400\n",
      "INFO:__main__:4: [Epoch 0]  duration(57.49586707499975)  loss:5.767154216766357 total-samples: 6400\n",
      "INFO:__main__:2: [Epoch 0]  duration(57.497045193000304)  loss:5.684810638427734 total-samples: 6400\n",
      "INFO:__main__:1: [Epoch 0]  duration(57.49853723899969)  loss:5.811939239501953 total-samples: 6400\n",
      "INFO:__main__:0: [Epoch 0]  duration(57.499225016999844)  loss:5.770937919616699 total-samples: 6400\n",
      "INFO:__main__:2: [Epoch 0]  duration(57.3714186279999)  loss:3.925326347351074 total-samples: 12800\n",
      "INFO:__main__:1: [Epoch 0]  duration(57.37188071199989)  loss:3.969285726547241 total-samples: 12800\n",
      "INFO:__main__:0: [Epoch 0]  duration(57.373041275000105)  loss:3.922102451324463 total-samples: 12800\n",
      "INFO:__main__:3: [Epoch 0]  duration(57.37743352200005)  loss:3.7698488235473633 total-samples: 12800\n",
      "INFO:__main__:7: [Epoch 0]  duration(57.375299133000226)  loss:3.8675193786621094 total-samples: 12800\n",
      "INFO:__main__:6: [Epoch 0]  duration(57.37926719500001)  loss:3.9498977661132812 total-samples: 12800\n",
      "INFO:__main__:5: [Epoch 0]  duration(57.381546716999765)  loss:3.8839111328125 total-samples: 12800\n",
      "INFO:__main__:4: [Epoch 0]  duration(57.384796402999655)  loss:3.7828776836395264 total-samples: 12800\n",
      "INFO:__main__:3: [Epoch 0]  duration(57.56508574600002)  loss:0.4888748526573181 total-samples: 19200\n",
      "INFO:__main__:2: [Epoch 0]  duration(57.572057309999764)  loss:0.5112927556037903 total-samples: 19200\n",
      "INFO:__main__:7: [Epoch 0]  duration(57.568645871999706)  loss:0.4705663323402405 total-samples: 19200\n",
      "INFO:__main__:1: [Epoch 0]  duration(57.57279408600016)  loss:0.48321157693862915 total-samples: 19200\n",
      "INFO:__main__:0: [Epoch 0]  duration(57.56971709100026)  loss:0.5088376402854919 total-samples: 19200\n",
      "INFO:__main__:6: [Epoch 0]  duration(57.56528169000012)  loss:0.49410247802734375 total-samples: 19200\n",
      "INFO:__main__:5: [Epoch 0]  duration(57.564610613999776)  loss:0.45060762763023376 total-samples: 19200\n",
      "INFO:__main__:4: [Epoch 0]  duration(57.560950141000376)  loss:0.48489123582839966 total-samples: 19200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!az batchai job file stream -w $WORKSPACE -e $EXPERIMENT --j $JOB_NAME --output-directory-id stdouterr -f stdout.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "tags": [
     "stripout"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mFile found with URL \"https://batchdtdemost.file.core.windows.net/batchdtdemoshare/edf507a2-6235-46c5-b560-fd463ba2e771/batchdtdemorg/workspaces/workspace/experiments/distributed_pytorch_v100/jobs/pytorch-horovod-8/43d5d58c-2ecd-4aa4-a459-93be2f302b7e/stdouterr/stderr.txt?sv=2016-05-31&sr=f&sig=9HSNbBWc0aGcQINWHJz508JAKw935Miy%2BkMwEj184NQ%3D&se=2018-12-17T14%3A51%3A44Z&sp=rl\". Start streaming\u001b[0m\n",
      "Warning: Permanently added '[10.0.0.6]:23' (ECDSA) to the list of known hosts.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!az batchai job file stream -w $WORKSPACE -e $EXPERIMENT --j $JOB_NAME --output-directory-id stdouterr -f stderr.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either wait for the job to complete or delete it with the command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az batchai job delete -w $WORKSPACE -e $EXPERIMENT --name $JOB_NAME -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean_up'></a>\n",
    "## Clean Up Resources\n",
    "Next we wish to tidy up the resource we created.  \n",
    "First we reset the default values we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az configure --defaults group=''\n",
    "!az configure --defaults location=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next we delete the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az batchai cluster delete -w $WORKSPACE --name $CLUSTER_NAME -g $GROUP_NAME -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cluster is deleted you will not incur any cost for the computation but you can still retain your experiments and workspace. If you wish to delete those as well execute the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az batchai experiment delete -w $WORKSPACE --name $EXPERIMENT -g $GROUP_NAME -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az batchai workspace delete -n $WORKSPACE -g $GROUP_NAME -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can delete the group and we will have deleted everything created for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[0minished .."
     ]
    }
   ],
   "source": [
    "!az group delete --name $GROUP_NAME -y"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.3",
    "jupytext_version": "0.8.6"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
